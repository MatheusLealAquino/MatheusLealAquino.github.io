---
layout: post
title: Executando uma IA localmente
author: Matheus Leal
date: '2024-11-05 09:02:00 +3'
category: IA, Ollama
summary: Executando uma IA localmente
thumbnail: ollama.png
---

Olá,

Escrevo este post para trazer um breve tutorial sobre como rodar uma IA generativa diretamente no seu computador.

Uma das opções disponíveis é o [Ollama](https://ollama.com/), que permite rodar modelos de LLM localmente. Existem diversos modelos que você pode testar, inclusive com a possibilidade de uso em máquinas com recursos limitados.

Para facilitar, podemos utilizar o modelo `llama 3.2` com 3 bilhões de parâmetros, que é leve e adequado para uso local.

### Como instalar o Ollama
Para instalar o Ollama, acesse: [https://ollama.com/download](https://ollama.com/download).

### Baixando o modelo
Use o comando a seguir para baixar o modelo `llama 3.2`:
```
ollama pull llama3.2
```

### Executando o modelo

- **Modo 1:**
    - Execute no terminal: ```ollama run llama3.2```
    - A partir daí, você pode interagir com a interface de chat disponível.

- **Modo 2:**
    - Execute via HTTP com o comando `curl`:
    ```
    curl --location 'http://localhost:11434/api/generate' \
    --header 'Content-Type: application/json' \
    --data '{
        "model": "llama3.2",
        "prompt": "Qual a capital do Brasil?",
        "stream": false
    }'
    ```

Seguindo esses passos simples, você já poderá interagir com uma IA localmente. Em breve, publicarei um novo post explicando como realizar uma iteração com essa interface para criar um RAG (Geração Aumentada por Recuperação).

Até o próximo post!
